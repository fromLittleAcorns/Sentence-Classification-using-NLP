{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook to look at a how to use a very simple bag of words approach to catagorise sentances.\n",
    "This is to help with analysing and assessing transaction data from a company based upon the \n",
    "description of the transaction.\n",
    "\n",
    "It is not anticipated that this will work very well, since the approach below does not include \n",
    "any use of word vectors and hence the dictionary very quickly becomes very large and filled \n",
    "with words that should be related but can't be with the model being used. Hence this is just a \n",
    "stepping stone to a more sophisticated model.\n",
    "\n",
    "The approach followed is largely that defined by rgunthrie in the Github repository:\n",
    "https://github.com/rguthrie3/DeepLearningForNLPInPytorch\n",
    ", although several functions have been copied from the example by Alexander Rakhlin in \n",
    "'CNN-for-sentance classification-in -keras'\n",
    "\n",
    "I have also looked at Gensim, and am likely to adopt some of their software for the next \n",
    "version of the analysis\n",
    "\n",
    "The present analysis gets an accuracy of 88.9% on training data but this falls to 66.2% on \n",
    "the test data.  I beleive this is largely due to additional geographic words coming up in the \n",
    "test data that are not included in the training data, and with no word vectors, there is no easy\n",
    "way to relate such words together\n",
    "\n",
    "An improvement I would like to try at some point is to use the natural language tool kit and \n",
    "stemmer from the nltk library, however, I think this model is fundamentally limited and so won't \n",
    "do so with this version\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%qtconsole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# System parameters\n",
    "my_file_path='johnrichmond/Dropbox/Machine Learning/text classification/Andrew/'\n",
    "csv_file_name='Payment items.csv'\n",
    "\n",
    "stop_list= set(\"for a c do h i if is it in g o p or e r t 's of the and mr ms to nd we\".split())\n",
    "\n",
    "\n",
    "remove_single_words=True\n",
    "use_subset_data=True\n",
    "max_cases=10000\n",
    "\n",
    "# only there are catagories 1-14 are valid, all others should be rejected\n",
    "\n",
    "min_cat=0\n",
    "max_cat=13\n",
    "num_cat=max_cat+1\n",
    "# Note actual catagory labels have been removed for reasons of commercial sensitivity\n",
    "label_to_idx={\"Cat 0\":0, \n",
    "              \"Cat 1\": 1,\n",
    "              \"Cat 2\":2,\n",
    "              \"Cat 3\": 3,\n",
    "              \"Cat 4\": 4,\n",
    "              \"Cat 5\":5,\n",
    "              \"Cat 6\": 6,\n",
    "              \"Cat 7\":7,\n",
    "              \"Cat 8\":8,\n",
    "              \"Cat 9\":9,\n",
    "              \"Cat 10\":10,\n",
    "              \"Cat 11\":11,\n",
    "              \"Cat 12\":12,\n",
    "              \"Cat 13\":13\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "no_epochs=20\n",
    "lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "replace_list={\"years\":\"year\", \"yr\":\"year\", \"wks\":\"week\",\"tickets\": \"ticket\",\n",
    "              \"terms\":\"term\", \"students\":\"student\",\"pupils\":\"pupil\",\"meals\": \"meal\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make the file path the same whether using Mac or Linux\n",
    "if sys.platform == 'darwin':\n",
    "    start='/Users/'\n",
    "else: start='/home/'\n",
    "    \n",
    "file_name=my_file_path+csv_file_name\n",
    "txt_file=start+file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Some functions that are needed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def is_integer(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        print s\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    #string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9,!?\\'\\`]\", \" \", string)\n",
    "    # matches any single character not in the above list and replaces with a white space\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    # adds a space prior to '\\apostrophy with an s\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    # adds a space before apostrphy with ve\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    # as above but for a 't\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    # as above but with 're\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    # not sure why this is necessary since I have not seen this option before\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    # adds space before and after commas\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    # adds spaces before and after the brackets\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    # replaces multiple white space sections with single whitespace\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rem_numbers(string):\n",
    "    string=re.sub(r\"[0123456789]\",\"\",string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string=string.strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(sentance, stop_list):\n",
    "    filtered = [word for word in sentance if word not in stop_list] \n",
    "    return filtered\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def replace_similar_words(sentance, replace_list):\n",
    "    new_sentance=[]\n",
    "    for word in sentance:\n",
    "        if word in replace_list:\n",
    "            word=replace_list[word]\n",
    "        new_sentance.append(word)\n",
    "    return new_sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    # The next line is not needed in this case since it is done prior to the call\n",
    "    #_, pred = output.topk(maxk, 1, True, True) # topk is torch function to return highest values in array\n",
    "    pred = output.t()  #Transpose\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "       #Note - the expand is a torch command to expend one tensor to the size of another\n",
    "       # target os a one D tensor. target.view(1,-1) reshapes the tensor.  The -1 means this is chosem\n",
    "       # by the software to get the right total size.  The first 1 indicates the number of rows to use\n",
    "       # The net outcome is an array with one column of length maxk for each target value.  The entire column \n",
    "       # is filled with the target value to facilitate easy comparison.\n",
    "       # The correct array then contains an array with true wherever the target value matches the prediction\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)    # The nomeclature[:k] returns the top k rows Since\n",
    "                                                           # there is no second array we get the every column.\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(txt_file,'rU') as file_obj:\n",
    "    data=[]\n",
    "    num=0\n",
    "    lines=[]\n",
    "    reader=csv.reader(file_obj)\n",
    "    for line in reader:\n",
    "        if reader.line_num<>1:\n",
    "    #       with col in line:\n",
    "            text_str=line[0]\n",
    "            catagory=line[15]\n",
    "            if is_integer(catagory):\n",
    "                data.append([text_str,int(catagory)])\n",
    "                num=num+1\n",
    "        if reader.line_num>max_cases and use_subset_data: break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# All data loaded, will now process each line of data\n",
    "# Initial work will focus upon removing numbers and punctuation\n",
    "sentances=[]\n",
    "catagories=[]\n",
    "catagory=[]\n",
    "identity=np.identity(num_cat)\n",
    "for row in data:\n",
    "    row[0]=clean_str(row[0])\n",
    "    row[0]=rem_numbers(row[0])# Done separately since I might not always want to do this\n",
    "\n",
    "    if row[0] in (None, \"\"):\n",
    "        # row rejected\n",
    "        continue\n",
    "    elif row[1] <1 or row[1]> num_cat or row[1] in (None, \"\"):\n",
    "        continue\n",
    "    sentance=row[0].split(\" \")\n",
    "    # remove stop list words\n",
    "    sentance=remove_stop_words(sentance,stop_list)\n",
    "    sentance=replace_similar_words(sentance,replace_list)\n",
    "    if len(sentance)==0: continue\n",
    "    row[1]=row[1]-1   \n",
    "    \n",
    "    # Getting to this point implies the row is ok and still has valid words, therefore will add\n",
    "    sentances.append(sentance)\n",
    "    catagories.append(identity[row[1]-1,:])\n",
    "    catagory.append(row[1])\n",
    "    \n",
    "# Remove single words  \n",
    "if remove_single_words==True:\n",
    "    final_sentances=[]\n",
    "    final_catagories=[]\n",
    "    final_catagory=[]\n",
    "    word_counts = Counter(itertools.chain(*sentances))\n",
    "    new_sentances=[[word for word in sentance if word_counts[word]>1] \n",
    "                    for sentance in sentances]\n",
    "    #Remove empty entries from both sentances and catagories\n",
    "    for index,sentance in enumerate(new_sentances):\n",
    "        if len(sentance)<>0:\n",
    "            final_sentances.append(sentance)\n",
    "            final_catagories.append(catagories[index])\n",
    "            final_catagory.append(catagory[index])\n",
    "    sentances=final_sentances\n",
    "    catagories=final_catagories\n",
    "    catagory=final_catagory\n",
    "            \n",
    "# note - might have to pad the sentances in future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab, vocab_inv=build_vocab(sentances)\n",
    "VOCAB_SIZE=len(vocab)\n",
    "word_to_ix=vocab\n",
    "dataset_total=len(sentances)\n",
    "train_data_max=int(dataset_total*75/100)\n",
    "test_data_start=train_data_max+1\n",
    "test_data_end=dataset_total-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Simple BOW classifier\n",
    "class BoWClassifier(nn.Module): # inheriting from nn.Module!\n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "def make_target(label, label_to_idx):\n",
    "    return torch.LongTensor([label_to_idx[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = BoWClassifier(num_cat, VOCAB_SIZE)\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function of a module,\n",
    "# which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the Pytorch devs, your module (in this case, BoWClassifier)\n",
    "# will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# To run the model, pass in a BoW vector, but wrapped in an autograd.Variable\n",
    "sample = sentances[0]\n",
    "bow_vector = make_bow_vector(sample, word_to_ix)\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ntopk=3\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr)\n",
    "predict=torch.LongTensor(train_data_max,ntopk).zero_()\n",
    "targets=torch.LongTensor(catagory[0:train_data_max])\n",
    "\n",
    "\n",
    "for epoch in xrange(no_epochs):\n",
    "    for i in xrange(train_data_max):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.  We need to clear them out\n",
    "        # before each instance\n",
    "        model.zero_grad()\n",
    "    \n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a Variable\n",
    "        # as an integer.  For example, if the target is SPANISH, then we wrap the integer\n",
    "        # 0.  The loss function then knows that the 0th element of the log probabilities is\n",
    "        # the log probability corresponding to SPANISH\n",
    "        bow_vec = autograd.Variable(make_bow_vector(sentances[i], word_to_ix))\n",
    "        myVar=torch.LongTensor([catagory[i]])\n",
    "        target = autograd.Variable(torch.LongTensor([catagory[i]]))\n",
    "    \n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "        _,ind=log_probs.data.topk(3,1,True,True)\n",
    "        predict[i,:]=ind\n",
    "    \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by calling\n",
    "        # optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print \"Epoch= {:d}, loss= {:s}\".format(epoch,loss)\n",
    "    \n",
    "    # Access training accuracy\n",
    "    #for i in xrange(0,train_data_max):\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "train_res=accuracy(predict,targets,topk=(1,3))\n",
    "print \"Training accuracy: \", train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "no_cases=test_data_end-test_data_start+1\n",
    "predict_test=torch.LongTensor(no_cases,ntopk).zero_()\n",
    "targets_test=torch.LongTensor(catagory[test_data_start:test_data_end+1])\n",
    "for i in xrange(test_data_start,test_data_end+1):\n",
    "    bow_vec = autograd.Variable(make_bow_vector(sentances[i], word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    _,ind=log_probs.data.topk(3,1,True,True)\n",
    "    predict_test[(i-test_data_start),:]=ind\n",
    "    #print log_probs\n",
    "#print next(model.parameters()) # Index corresponding to Spanish goes up, English goes down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "test_res=accuracy(predict_test,targets_test,topk=(1,3))\n",
    "print \"Test accuracy: \", test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
